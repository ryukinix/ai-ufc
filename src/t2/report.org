#+STARTUP: showall align
#+OPTIONS: todo:nil tasks:("IN-PROGRESS" "DONE") tags:nil
#+AUTHOR: Manoel Vilela / 394192
#+TITLE: Inteligência Computacional @@latex:\\@@ TR2 - Redes Neurais
#+EXCLUDE_TAGS: TOC_3
#+LANGUAGE: bt-br
#+LATEX_CLASS: report
#+LATEX_HEADER: \usepackage[]{babel}
#+LATEX_HEADER: \usepackage{indentfirst}
#+LATEX_HEADER: \renewcommand\listingscaption{Código}
#+OPTIONS: toc:nil
[[./pics/ufc.png]]


#+BEGIN_abstract

Este trabalho tem como foco de estudo algumas Redes Neurais
Artificiais. Durante o desenvolvimento foram testado certos algoritmos
bastante famosos como Perceptron e Multi-Layer Perceptron
(MLP). Adicionalmente é também explorado uma rede exótica com nome de
/Extreme Learning Machine/ (ELM) , ou /Random Vector Functional Link/
(RVFL) que possui como elemento do seu treinamento o famoso /Ordinary
Leasting Squares/ (OLS).

#+END_abstract
#+TOC: headlines=2

* Introdução

Os algoritmos para esse trabalho foram desenvolvidos na linguagem
Python3 e testado especificamente na versão 3.6 e 3.7. O Sistema
Operacional utilizado foi GNU/Linux e o editor principal Emacs
25.2.2. As bibliotecas utilizadas estão listadas no arquivo
requirements.txt onde podem ser instaladas pelo gerenciador de pacotes
~pip~ via:

#+BEGIN_EXAMPLE
pip install -r requirements.txt
#+END_EXAMPLE

Embora há mais de dois arquivos Python, as questões estão resolvidas
como ponto de entrada nos arquivos ~q1.py~ e ~q2.py~ e podem ser
executadas como:

#+BEGIN_EXAMPLE
python q1.py
python q2.py
#+END_EXAMPLE

O restante dos arquivos são modulos para re-utilização de código entre
os diferentes problemas enfrentados no desenvolvimento do trabalho. As
redes neurais que foram estudadas, codificadas ou experimentadas são:

+ Extreme Learning Machine (ELM)
+ Perceptron
+ Multi-Layer Perceptron (MLP)

Como critérios de avaliação dos modelos foram usados:

+ K-Fold
+ Hold Out
+ Leave One Out
+ \( R^{2} \) (Métrica de regressão)
+ Acurácia (Métrica de classificação)

* Questão 1

Este é um problema de regressão, portanto, não irei utilizar a função
de ativação /sigmoid/ na camada de saída. Ao avaliar algumas características do
/dataset/ aerogerador, é perceptível que a variável \(y\) possui uma
alta variância \( \sigma^{2}_{y} =11125.10 \) e um comportamento
não-linear nas extremidades.

Para o caso mais simples onde as características da rede é apenas um
vetor simples \(\vec{x})\) que representa a velocidade do vento em \(
m/s) \), a rede neural possuiu melhores resultados para poucos
neurônios ocultos, parametrizados pela leta \(q\). A seguir é possível
ver uma síntese das respostas:

#+CAPTION: Regressão linear com variados neurônios ocultos.
[[file:pics/q1-elm-linear.png]]

Esta resposta demonstrou-se um tanto curiosa, pois eu (relator)
esperava que com o acréscimo de neurônios ocultos fosse possível a
rede possuir um comportamento não-linear na saída, mas isso não é o
que este resultado demonstra.

Dado esse fato, conclui que para alcançar este efeito na cruva seria
necessário então realizar uma regressão polinomial aumentando a
quantidade de /features/ recebidas da rede como potenciações do vetor
\( x \) sendo ~k~ o grau do polinômio de regressão.

#+BEGIN_LaTeX latex
\centering
\begin{equation*}

\bold{X} = \left[
\bold{-1} \quad |
\quad \bold{x}^1 \quad |
\quad \bold{x}^2 \quad |
\quad ... \quad |
\quad \bold{x}^k
\right]

\end{equation*}


#+END_LaTeX


Ao realizar alguns experimentos com essa estrutura para k > 1, tem-se
os resultados a seguir:

#+CAPTION: Regressão polinomial com variados neurônios ocultos.
[[file:pics/q1-elm-polinomial.png]]




* Questão 2
